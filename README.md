# Data Warehouse ETL Pipeline Implementation (AWS S3 -> Redshift)

## Summary

This project is a ETL process built for a music streamming company moving their growing data to cloud. The company finished loading their data to AWS S3 storage folder. This project will handle extracting data from file folders (a serial of JSON files) to a AWS Redshift cluster. In this process, we first designed new data warehouse structure with fact and demension tables to store tranformed data. Data was loaded to staging tables in raw and then transformed before loaded into target tables.

## Data Source

There are two types of data related to this project: Song Dataset and Log Dataset

* **Song** Dataset: <br>
    a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

        song_data/A/B/C/TRABCEI128F424C983.json
        song_data/A/A/B/TRAABJL12903CDCF1A.json

    Inside each JSON file, data has following structure: 
        
        {
          "num_songs": 1,
          "artist_id": "ARJIE2Y1187B994AB7",
          "artist_latitude": null,
          "artist_longitude": null,
          "artist_location": "",
          "artist_name": "Line Renaud",
          "song_id": "SOUPIRU12A6D4FA1E1",
          "title": "Der Kleine Dompfaff",
          "duration": 152.92036,
          "year": 0
        }
<br>

* **Log** Dataset: <br>
    For illustration purpose, the second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

    Fore each log data file path, it was partitioned by year, month, date like following:

        log_data/2018/11/2018-11-12-events.json
        log_data/2018/11/2018-11-13-events.json

    Example of data format for each log file:<br>
    JSON view:
    
        {
            "artist": null,
            "auth": "Logged In",
            "firstName": "Walter",
            "gender": "M",
            "itemInSession": 0,
            "lastName": "Frye",
            "length": null,
            "level": "free",
            "location": "San Francisco-Oakland-Hayward, CA",
            "method": "GET",
            "page": "Home",
            "registration": 1540919166796.0,
            "sessionId": 38,
            "song": null,
            "status": 200,
            "ts": 1541105830796,
            "userAgent": "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"",
            "userId": "39"
        }
   <br>        
    Tablar view: <br>
   
   <img src="./image/log-data_sample.png" width="1500">
   
## Data Warehouse Table Design 

We plan to create a database include staging tables without any keys and a star schema with relationships between fact and dimension tables 

* Redshift Cluster Database Entity Diagram

 <img src="./image/ERD.png" width="1500">
 
* Distribution Key and Sort Key

Here the distribution key was set on the songplay (Fact) table song_id column. The sort keys are set on each dimension table primary key. 

* Analytics Query



## ETL pipeline



    
   
   

