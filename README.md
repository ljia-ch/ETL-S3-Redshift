# Data Warehouse ETL Pipeline Implementation (AWS S3 -> Redshift)

## Summary

This project is a ETL process built for a music streamming company moving their growing data to cloud. The company finished loading their data to AWS S3 storage folder. This project will handle extracting data from file folders (a serial of JSON files) to a AWS Redshift cluster. In this process, we first designed new data warehouse structure with fact and demension tables to store tranformed data. Data was loaded to staging tables in raw and then transformed before loaded into target tables.

## Data Source

There are two types of data related to this project: Song Dataset and Log Dataset

* **Song** Dataset: <br>
    A subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

        song_data/A/B/C/TRABCEI128F424C983.json
        song_data/A/A/B/TRAABJL12903CDCF1A.json

    Inside each JSON file, data has following structure: 
        
        {
          "num_songs": 1,
          "artist_id": "ARJIE2Y1187B994AB7",
          "artist_latitude": null,
          "artist_longitude": null,
          "artist_location": "",
          "artist_name": "Line Renaud",
          "song_id": "SOUPIRU12A6D4FA1E1",
          "title": "Der Kleine Dompfaff",
          "duration": 152.92036,
          "year": 0
        }
<br>

* **Log** Dataset: <br>
    For illustration purpose, the second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

    Fore each log data file path, it was partitioned by year, month, date like following:

        log_data/2018/11/2018-11-12-events.json
        log_data/2018/11/2018-11-13-events.json

    Example of data format for each log file:<br>
    JSON view:
    
        {
            "artist": null,
            "auth": "Logged In",
            "firstName": "Walter",
            "gender": "M",
            "itemInSession": 0,
            "lastName": "Frye",
            "length": null,
            "level": "free",
            "location": "San Francisco-Oakland-Hayward, CA",
            "method": "GET",
            "page": "Home",
            "registration": 1540919166796.0,
            "sessionId": 38,
            "song": null,
            "status": 200,
            "ts": 1541105830796,
            "userAgent": "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"",
            "userId": "39"
        }
   <br>        
    Tablar view with top lines: <br>
   
   <img src="./image/log-data_sample.png" width="1500">

## Data Warehouse Table Design 

We plan to create a database include staging tables without any keys and a star schema with fact and dimension tables and relationships in between.

* Redshift Cluster Database Entity Diagram: tables marked in different colors to differentiate staging tables from star schema tables.

 <img src="./image/ERD.png" width="1500">

* Table Creation:
    * sql_queries.py: includes all staging, dimension and fact tables' creation and records insert queries
    * Distribution and Sort Key in fact and dimensions. 
        * The distribution key was set on the songplays (Fact) table (song_id column). 
        * The sort keys are set on each dimension table primary key for easy join. 
        * In songplays table, sort key are composited by the 4 foreign keys.

## ETL Process
* Load data to taging tables: after table creation, we are ready to load data. Here we use COPY command load data from S3 file folder to staging tables.
* Load data to star schema:
    Use sql queries
    * User and Time tables: records are inserted from staging_events table.
    * Song and Artist tables: data are inserted from staging_songs table.
    * Songplays table: records are inserted by joining staging_events and staging_songs table.










    
   
   

